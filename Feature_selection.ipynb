{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hydraulic-captain",
   "metadata": {},
   "source": [
    "# Comprehensive Guide on Feature Selection\n",
    "Hello friends,\n",
    "\n",
    "<b>Feature Selection</b> is the process of selecting optimal number of features from a larger set of features. There are several advantages of this feature selection process and also there are various techniques available for this feature selection process. \n",
    "\n",
    "So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-syndicate",
   "metadata": {},
   "source": [
    "<b>This Notebook is based on Soledad Galli's course</b> - Feature Selection for Machine Learning https://www.udemy.com/course/feature-selection-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-marijuana",
   "metadata": {},
   "source": [
    "She had done a fabulous job in her above course wherein she had put all the major feature selection techniques together at one place. I have adapted code and instructions from her course in this Notebook. I like to congratulate her for her excellent work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-charger",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Introduction to Feature Selection\n",
    "\n",
    "2. Filter Methods\n",
    "\n",
    "    2.1. Basic Methods\n",
    "        2.1.1 Remove Constant Features\n",
    "        2.1.2 Remove Quasi-Constant Features\n",
    "    2.2 Univariate Selection Methods\n",
    "        2.2.1 SelectKBest\n",
    "        2.2.2 SelectPercentile\n",
    "    2.3 Information Gain.\n",
    "    \n",
    "    2.4 Fisher Score (chi-square implementation)\n",
    "    \n",
    "    2.5 ANOVA F-Value for Feature Selection\n",
    "    \n",
    "    2.6 Correlation-Matrix with Heatmap\n",
    "    \n",
    "    \n",
    "3. Wrapper Methods\n",
    "\n",
    "    3.1 Forward Selection\n",
    "    \n",
    "    3.2 Backward Elimination\n",
    "    \n",
    "    3.3 Exhaustive Feature Selection\n",
    "    \n",
    "    3.4 Recursive Feature Elimination\n",
    "    \n",
    "    3.5 Recursive Feature Elimination with Cross-Validation\n",
    "    \n",
    "\n",
    "4. Embedded Methods\n",
    "\n",
    "    4.1 LASSO Regression\n",
    "    \n",
    "    4.2 Random Forest Importance\n",
    "\n",
    "5. How to choose the right feature selection method\n",
    "\n",
    "6. Tips and tricks for feature selection\n",
    "\n",
    "7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-intranet",
   "metadata": {},
   "source": [
    "## Feature Selection – Techniques\n",
    "\n",
    " *Feature selection techniques are categorized into 3 typers. These are as follows:-\n",
    "\n",
    "1. Filter methods\n",
    "2. Wrapper methods\n",
    "3. Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-frontier",
   "metadata": {},
   "source": [
    "## Filter Methods\n",
    "\n",
    "<b>Filter methods</b> \n",
    "    consists of various techniques as given below:-\n",
    "\n",
    "1. Basic methods\n",
    "2. Univariate methods\n",
    "3. Information gain\n",
    "4. Fischer score\n",
    "5. Correlation Matrix with Heatmap\n",
    "\n",
    "## Wrapper Methord\n",
    "<b>Wrapper Methods</b>\n",
    "Wrapper methods consists of the following techniques:-\n",
    "\n",
    "1. Forward Selection\n",
    "2. Backward Elimination\n",
    "3. Exhaustive Feature Selection\n",
    "4. Recursive Feature Elimination\n",
    "5. Recursive Feature Elimination with Cross-Validation\n",
    "\n",
    "## Embedded Methord\n",
    "<b>Embedded Methods</b>\n",
    "Embedded methods consists of the following techniques:-\n",
    "\n",
    "1. LASSO\n",
    "2. RIDGE\n",
    "3. Tree Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-category",
   "metadata": {},
   "source": [
    "## 2. Filter Methods \n",
    "\n",
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The characteristics of these methods are as follows:-\n",
    "\n",
    "\n",
    "* These methods rely on the characteristics of the data (feature characteristics)\n",
    "* They do not use machine learning algorithms.\n",
    "* These are model agnostic.\n",
    "* They tend to be less computationally expensive.\n",
    "* They usually give lower prediction performance than wrapper methods.\n",
    "* They are very well suited for a quick screen and removal of irrelevant features.\n",
    "\n",
    "Filter methods consists of various techniques as given below:-\n",
    "\n",
    "* 2.1. Basic methods\n",
    "* 2.2. Univariate feature selection\n",
    "* 2.3. Information gain\n",
    "* 2.4. Fischer score\n",
    "* 2.5. ANOVA F-Value for Feature Selection\n",
    "* 2.6. Correlation Matrix with Heatmap\n",
    "\n",
    "Filter methods can be explained with the help of following graphic:\n",
    "\n",
    "## 2.1 Basic methods \n",
    "\n",
    "* Under basic methods, we remove constant and quasi-constant features.\n",
    "\n",
    "### 2.1.1 Remove constant features \n",
    "\n",
    "\n",
    "* <b>Constant features are those that show the same value, just one value, for all the observations of the dataset. This is, the same value for all the rows of the dataset.</b> These features provide no information that allows a machine learning model to discriminate or predict a target.\n",
    "\n",
    "* Identifying and removing constant features, is an easy first step towards feature selection and more easily interpretable machine learning models. To identify constant features, we can use the VarianceThreshold function from sklearn.\n",
    "\n",
    "* I will demonstrate how to identify constant features using the Santander Customer Satisfaction dataset from Kaggle.\n",
    "\n",
    "<h>Source :</h>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heard-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train = pd.read_csv(r'santander-customer-satisfaction_train.csv',nrows=35000)\n",
    "X_test  = pd.read_csv(r'santander-customer-satisfaction_test.csv',nrows=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "protecting-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop TARGET label from X_train\n",
    "\n",
    "X_train.drop(labels = ['TARGET'],axis =1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "shared-personality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 370), (35000, 370))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-finger",
   "metadata": {},
   "source": [
    "<b>Important :- </b>\n",
    "\n",
    "* In all feature selection procedures, it is good practice to select the features by examining only the training set. This is done to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-observation",
   "metadata": {},
   "source": [
    "<b>Using variance threshold from sklearn</b>\n",
    "* Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "entitled-policy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "sel.fit(X_train) # fit finds the features with zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indie-palmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_support is a boolean vector that indicates which features are retained\n",
    "# if we sum over get_support, we get the number of features that are not constant\n",
    "\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "plastic-cradle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ind_var2_0',\n",
       " 'ind_var2',\n",
       " 'ind_var18_0',\n",
       " 'ind_var18',\n",
       " 'ind_var27_0',\n",
       " 'ind_var28_0',\n",
       " 'ind_var28',\n",
       " 'ind_var27',\n",
       " 'ind_var34_0',\n",
       " 'ind_var34',\n",
       " 'ind_var41',\n",
       " 'ind_var46_0',\n",
       " 'ind_var46',\n",
       " 'num_var18_0',\n",
       " 'num_var18',\n",
       " 'num_var27_0',\n",
       " 'num_var28_0',\n",
       " 'num_var28',\n",
       " 'num_var27',\n",
       " 'num_var34_0',\n",
       " 'num_var34',\n",
       " 'num_var41',\n",
       " 'num_var46_0',\n",
       " 'num_var46',\n",
       " 'saldo_var18',\n",
       " 'saldo_var28',\n",
       " 'saldo_var27',\n",
       " 'saldo_var34',\n",
       " 'saldo_var41',\n",
       " 'saldo_var46',\n",
       " 'delta_imp_amort_var18_1y3',\n",
       " 'delta_imp_amort_var34_1y3',\n",
       " 'imp_amort_var18_hace3',\n",
       " 'imp_amort_var18_ult1',\n",
       " 'imp_amort_var34_hace3',\n",
       " 'imp_amort_var34_ult1',\n",
       " 'imp_reemb_var13_hace3',\n",
       " 'imp_reemb_var17_hace3',\n",
       " 'imp_reemb_var33_hace3',\n",
       " 'imp_trasp_var17_out_hace3',\n",
       " 'imp_trasp_var33_out_hace3',\n",
       " 'num_var2_0_ult1',\n",
       " 'num_var2_ult1',\n",
       " 'num_reemb_var13_hace3',\n",
       " 'num_reemb_var17_hace3',\n",
       " 'num_reemb_var33_hace3',\n",
       " 'num_trasp_var17_out_hace3',\n",
       " 'num_trasp_var33_out_hace3',\n",
       " 'saldo_var2_ult1',\n",
       " 'saldo_medio_var13_medio_hace3',\n",
       " 'saldo_medio_var29_hace3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the constant features\n",
    "print(\n",
    "    len([\n",
    "        x for x in X_train.columns\n",
    "        if x not in X_train.columns[sel.get_support()]\n",
    "    ]))\n",
    "\n",
    "[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-jefferson",
   "metadata": {},
   "source": [
    "* We can see that there are 51 columns / variables that are constant.Means that 51 variable have same value for all observation intraining set\n",
    "\n",
    "* We then use the transform function to reduce the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "jewish-punishment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 319), (35000, 319))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can then drop these columns from the train and test sets\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "\n",
    "# check the shape of training and test set\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-bloom",
   "metadata": {},
   "source": [
    "### 2.1.2 Remove quasi-constant features \n",
    "\n",
    "\n",
    "* <b>Quasi-constant features are those that show the same value for the great majority of the observations of the dataset. In general, these features provide little if any information that allows a machine learning model to discriminate or predict a target.</b> But there can be exceptions. So we should be careful when removing these type of features. Identifying and removing quasi-constant features, is an easy first step towards feature selection and more easily interpretable machine learning models.\n",
    "\n",
    "\n",
    "* To identify quasi-constant features, we can once again use the VarianceThreshold function from sklearn.\n",
    "\n",
    "\n",
    "* Here I will demonstrate how to identify quasi-constant features using the Santander Customer Satisfaction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "amino-gross",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39205.170000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49278.030000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67333.770000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64007.970000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117310.979016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0   1     2     23                 0.0                      0.0   \n",
       "1   3     2     34                 0.0                      0.0   \n",
       "2   4     2     23                 0.0                      0.0   \n",
       "3   8     2     37                 0.0                    195.0   \n",
       "4  10     2     39                 0.0                      0.0   \n",
       "\n",
       "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "0                      0.0                      0.0                      0.0   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "3                    195.0                      0.0                      0.0   \n",
       "4                      0.0                      0.0                      0.0   \n",
       "\n",
       "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "0                      0.0                      0.0  ...   \n",
       "1                      0.0                      0.0  ...   \n",
       "2                      0.0                      0.0  ...   \n",
       "3                      0.0                      0.0  ...   \n",
       "4                      0.0                      0.0  ...   \n",
       "\n",
       "   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "0                      0.0                      0.0                     0.0   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "2                      0.0                      0.0                     0.0   \n",
       "3                      0.0                      0.0                     0.0   \n",
       "4                      0.0                      0.0                     0.0   \n",
       "\n",
       "   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "0                     0.0                      0.0                      0.0   \n",
       "1                     0.0                      0.0                      0.0   \n",
       "2                     0.0                      0.0                      0.0   \n",
       "3                     0.0                      0.0                      0.0   \n",
       "4                     0.0                      0.0                      0.0   \n",
       "\n",
       "   saldo_medio_var44_ult1  saldo_medio_var44_ult3          var38  TARGET  \n",
       "0                     0.0                     0.0   39205.170000       0  \n",
       "1                     0.0                     0.0   49278.030000       0  \n",
       "2                     0.0                     0.0   67333.770000       0  \n",
       "3                     0.0                     0.0   64007.970000       0  \n",
       "4                     0.0                     0.0  117310.979016       0  \n",
       "\n",
       "[5 rows x 371 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the Santander customer satisfaction dataset from Kaggle\n",
    "\n",
    "import pandas as pd\n",
    "X_train = pd.read_csv(r'santander-customer-satisfaction_train.csv',nrows=35000)\n",
    "X_test  = pd.read_csv(r'santander-customer-satisfaction_test.csv',nrows=15000)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "mighty-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the target variable\n",
    "\n",
    "X_train.drop(labels = ['TARGET'],axis =1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-offer",
   "metadata": {},
   "source": [
    "#### Removing quasi-constant features\n",
    "\n",
    "Using variance threshold from sklearn\n",
    "\n",
    "* <b>Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold.</b> By default, it removes all zero-variance features, i.e., features that have the same value in all samples.\n",
    "\n",
    "\n",
    "* Here, I will change the default threshold to remove almost / quasi-constant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "heavy-maple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0.01)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold = 0.01) # 0.01 indicates 99% of observations approximately\n",
    "\n",
    "sel.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "crude-daily",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_support is a boolean vector that indicates which features \n",
    "# are retained. If we sum over get_support, we get the number\n",
    "# of features that are not quasi-constant\n",
    "\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adaptive-nomination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ind_var1',\n",
       " 'ind_var2_0',\n",
       " 'ind_var2',\n",
       " 'ind_var6_0',\n",
       " 'ind_var6',\n",
       " 'ind_var13_largo',\n",
       " 'ind_var13_medio_0',\n",
       " 'ind_var13_medio',\n",
       " 'ind_var14',\n",
       " 'ind_var17_0',\n",
       " 'ind_var17',\n",
       " 'ind_var18_0',\n",
       " 'ind_var18',\n",
       " 'ind_var19',\n",
       " 'ind_var20_0',\n",
       " 'ind_var20',\n",
       " 'ind_var27_0',\n",
       " 'ind_var28_0',\n",
       " 'ind_var28',\n",
       " 'ind_var27',\n",
       " 'ind_var29_0',\n",
       " 'ind_var29',\n",
       " 'ind_var30_0',\n",
       " 'ind_var31_0',\n",
       " 'ind_var31',\n",
       " 'ind_var32_cte',\n",
       " 'ind_var32_0',\n",
       " 'ind_var32',\n",
       " 'ind_var33_0',\n",
       " 'ind_var33',\n",
       " 'ind_var34_0',\n",
       " 'ind_var34',\n",
       " 'ind_var40',\n",
       " 'ind_var41',\n",
       " 'ind_var39',\n",
       " 'ind_var44_0',\n",
       " 'ind_var44',\n",
       " 'ind_var46_0',\n",
       " 'ind_var46',\n",
       " 'num_var6_0',\n",
       " 'num_var6',\n",
       " 'num_var13_medio_0',\n",
       " 'num_var13_medio',\n",
       " 'num_var18_0',\n",
       " 'num_var18',\n",
       " 'num_op_var40_hace3',\n",
       " 'num_var27_0',\n",
       " 'num_var28_0',\n",
       " 'num_var28',\n",
       " 'num_var27',\n",
       " 'num_var29_0',\n",
       " 'num_var29',\n",
       " 'num_var33',\n",
       " 'num_var34_0',\n",
       " 'num_var34',\n",
       " 'num_var41',\n",
       " 'num_var46_0',\n",
       " 'num_var46',\n",
       " 'saldo_var18',\n",
       " 'saldo_var28',\n",
       " 'saldo_var27',\n",
       " 'saldo_var34',\n",
       " 'saldo_var41',\n",
       " 'saldo_var46',\n",
       " 'delta_imp_amort_var18_1y3',\n",
       " 'delta_imp_amort_var34_1y3',\n",
       " 'delta_imp_aport_var33_1y3',\n",
       " 'delta_num_aport_var33_1y3',\n",
       " 'imp_amort_var18_hace3',\n",
       " 'imp_amort_var18_ult1',\n",
       " 'imp_amort_var34_hace3',\n",
       " 'imp_amort_var34_ult1',\n",
       " 'imp_reemb_var13_hace3',\n",
       " 'imp_reemb_var17_hace3',\n",
       " 'imp_reemb_var33_hace3',\n",
       " 'imp_trasp_var17_out_hace3',\n",
       " 'imp_trasp_var33_out_hace3',\n",
       " 'ind_var7_emit_ult1',\n",
       " 'ind_var7_recib_ult1',\n",
       " 'num_var2_0_ult1',\n",
       " 'num_var2_ult1',\n",
       " 'num_aport_var33_hace3',\n",
       " 'num_aport_var33_ult1',\n",
       " 'num_var7_emit_ult1',\n",
       " 'num_meses_var13_medio_ult3',\n",
       " 'num_meses_var17_ult3',\n",
       " 'num_meses_var29_ult3',\n",
       " 'num_meses_var33_ult3',\n",
       " 'num_meses_var44_ult3',\n",
       " 'num_reemb_var13_hace3',\n",
       " 'num_reemb_var13_ult1',\n",
       " 'num_reemb_var17_hace3',\n",
       " 'num_reemb_var17_ult1',\n",
       " 'num_reemb_var33_hace3',\n",
       " 'num_reemb_var33_ult1',\n",
       " 'num_trasp_var17_in_hace3',\n",
       " 'num_trasp_var17_in_ult1',\n",
       " 'num_trasp_var17_out_hace3',\n",
       " 'num_trasp_var17_out_ult1',\n",
       " 'num_trasp_var33_in_hace3',\n",
       " 'num_trasp_var33_in_ult1',\n",
       " 'num_trasp_var33_out_hace3',\n",
       " 'num_trasp_var33_out_ult1',\n",
       " 'num_venta_var44_hace3',\n",
       " 'saldo_var2_ult1',\n",
       " 'saldo_medio_var13_medio_hace3',\n",
       " 'saldo_medio_var29_hace3']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally we can print the quasi-constant features\n",
    "print(\n",
    "    len([\n",
    "        x for x in X_train.columns\n",
    "        if x not in X_train.columns[sel.get_support()]\n",
    "    ]))\n",
    "\n",
    "[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-usage",
   "metadata": {},
   "source": [
    "* We can see that 107 columns / variables are almost constant.  means that 107 variables show predominantly one value for ~99% the observations of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "clean-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-cff1e02c98bc>:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train['ind_var31'].value_counts() / np.float(len(X_train))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.996286\n",
       "1    0.003714\n",
       "Name: ind_var31, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of observations showing each of the different values\n",
    "\n",
    "import numpy as np\n",
    "X_train['ind_var31'].value_counts() / np.float(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-sociology",
   "metadata": {},
   "source": [
    "* We can see that > 99% of the observations show one value, 0. Therefore, this feature is almost constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "iraqi-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then remove the features from training and test set\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "wired-sheep",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 263), (15000, 263))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of training and test set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-poison",
   "metadata": {},
   "source": [
    "* By removing constant and quasi-constant features, we reduced the feature space from 370 to 263. We can see that more than 100 features were removed from the present dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-petroleum",
   "metadata": {},
   "source": [
    "## 2.2 Univariate selection methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-format",
   "metadata": {},
   "source": [
    "* Univariate feature selection methods works by selecting the best features based on univariate statistical tests like ANOVA. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method.\n",
    "\n",
    "* The methods based on F-test estimate the degree of linear dependency between two random variables. They assume a linear relationship between the feature and the target. These methods also assume that the variables follow a Gaussian distribution.\n",
    "\n",
    "* There are 4 methods that fall under this category :-\n",
    "\n",
    "1. SelectKBest\n",
    "2. SelectPercentile\n",
    "3. SelectFpr, SelectFdr, or family wise error SelectFwe\n",
    "4. GenericUnivariateSelection\n",
    "\n",
    "<b>Source :</b> https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "* Here,limit the discussion to SelectKBest and SelectPercentile, because these two are most commonly used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-uncle",
   "metadata": {},
   "source": [
    "### 2.2.1 SelectKBest \n",
    "\n",
    "\n",
    "* This method select features according to the k highest scores.\n",
    "\n",
    "* For instance, we can perform a chi-square test to the samples to retrieve only the <b>two best features from iris dataset</b> as follows:\n",
    "\n",
    "<b>Source :</b> https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "comprehensive-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "following-system",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y # we can see 'Y' is a Categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "funny-humanity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the two best features\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X,y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-research",
   "metadata": {},
   "source": [
    "* now we have selected 2 best features from iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-hammer",
   "metadata": {},
   "source": [
    "### 2.2.2 SelectPercentile \n",
    "\n",
    "* Select features according to a percentile of the highest scores.\n",
    "\n",
    "<b>Source :</b> https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "french-preserve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectPercentile , chi2\n",
    "\n",
    "X,y = load_digits(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "surprised-devon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 7)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the features based on top 10 percentile.\n",
    "\n",
    "X_new = SelectPercentile(chi2 , percentile=10).fit_transform(X,y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-channel",
   "metadata": {},
   "source": [
    "* We can see that only 7 features lie on the top 10 percentile and hence we select them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-johns",
   "metadata": {},
   "source": [
    "### Important information\n",
    "\n",
    "* These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile:\n",
    "\n",
    "* For regression tasks: f_regression, mutual_info_regression\n",
    "\n",
    "* For classification tasks: chi2, f_classif, mutual_info_classif\n",
    "\n",
    "The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\n",
    "\n",
    "### Feature selection with sparse data\n",
    "\n",
    "If you use sparse data (i.e. data represented as sparse matrices), chi2, mutual_info_regression, mutual_info_classif will deal with the data without making it dense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-sailing",
   "metadata": {},
   "source": [
    "### Warning\n",
    "\n",
    "* Beware not to use a regression scoring function with a classification problem, you will get useless results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-moscow",
   "metadata": {},
   "source": [
    "## 2.3 Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-connecticut",
   "metadata": {},
   "source": [
    "* <b>Information gain</b> or <b>mutual information</b> measures how much information the presence/absence of a feature contributes to making the correct prediction on the target\n",
    "\n",
    "<b>Mutual information measures the information that X and Y share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if X and Y are independent, then knowing X does not give any information about Y and vice versa, so their mutual information is zero. At the other extreme, if X is a deterministic function of Y and Y is a deterministic function of X then all information conveyed by X is shared with Y: knowing X determines the value of Y and vice versa. As a result, in this case the mutual information is the same as the uncertainty contained in Y (or X) alone, namely the entropy of Y (or X). Moreover, this mutual information is the same as the entropy of X and as the entropy of Y. (A very special case of this is when X and Y are the same random variable.)</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-restriction",
   "metadata": {},
   "source": [
    "### mutual_info_classif\n",
    "\n",
    "* It estimates mutual information for a discrete target variable.\n",
    "\n",
    "* Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "\n",
    "* This function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "* It can be used for univariate features selection.\n",
    "\n",
    "### mutual_info_regression\n",
    "\n",
    "* Estimate mutual information for a continuous target variable.\n",
    "\n",
    "* Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "\n",
    "* The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "* It can be used for univariate features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-string",
   "metadata": {},
   "source": [
    "## 2.4 Fisher Score (chi-square implementation) \n",
    "\n",
    "\n",
    "* It is the chi-square implementation in scikit-learn. It computes chi-squared stats between each non-negative feature and class.\n",
    "\n",
    "* This score should be used to evaluate categorical variables in a classification task. It compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "comparable-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "adaptive-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# load iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create features and target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "# convert to categorical data by converting data to integers\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "daily-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "revolutionary-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "print('Original number of features:', X.shape[1])\n",
    "print('Reduced number of features:', X_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-maple",
   "metadata": {},
   "source": [
    "* We can see that the above code helps us to select the 2 best features based on Fisher score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-accordance",
   "metadata": {},
   "source": [
    "## 2.5 ANOVA F-value For Feature Selection \n",
    "\n",
    "\n",
    "* Compute the ANOVA F-value for the provided sample.\n",
    "\n",
    "* If the features are <b>categorical, we will calculate a chi-square statistic</b> between each feature and the target vector. However, if the features are <b>quantitative, we will compute the ANOVA F-value between each feature</b> and the target vector.\n",
    "\n",
    "* The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "unauthorized-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fancy-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# Create features and target\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "described-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Features With Best ANOVA F-Values\n",
    "\n",
    "# Create an SelectKBest object to select features with two best ANOVA F-Values\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "\n",
    "# Apply the SelectKBest object to the features and target\n",
    "X_kbest = fvalue_selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "undefined-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# View results\n",
    "print('Original number of features:', X.shape[1])\n",
    "print('Reduced number of features:', X_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-logic",
   "metadata": {},
   "source": [
    "* We can see that the above code helps us to select the 2 best features based on ANOVA F-Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-utilization",
   "metadata": {},
   "source": [
    "## 2.6 Correlation-Matrix with Heatmap \n",
    "\n",
    "\n",
    "* <b>Correlation</b> is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other.\n",
    "\n",
    "* <b>Good variables are highly correlated with the target.</b>\n",
    "\n",
    "* Correlated predictor variables provide redundant information.\n",
    "\n",
    "* <b>Variables should be correlated with the target but uncorrelated among themselves.</b>\n",
    "\n",
    "* Correlation Feature Selection evaluates subsets of features on the basis of the following hypothesis:\n",
    "\n",
    "    * \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\".\n",
    "\n",
    "\n",
    "* <b>Using Pearson correlation our returned coefficient values will vary between -1 and 1.</b>\n",
    "\n",
    "* If the correlation between two features is 0 this means that changing any of these two features will not affect the other.\n",
    "\n",
    "* If the correlation between two features is greater than 0 this means that increasing the values in one feature will make increase also the values in the other feature (the closer the correlation coefficient is to 1 and the stronger is going to be this bond between the two different features).\n",
    "\n",
    "* If the correlation between two features is less than 0 this means that increasing the values in one feature will make decrease the values in the other feature (the closer the correlation coefficient is to -1 and the stronger is going to be this relationship between the two different features).\n",
    "\n",
    "* In this analysis we will check if the selected variables are highly correlated with each other. If they are, we would then need to keep just one of the correlated ones and drop the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dutch-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Create features and target\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "close-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3\n",
      "0    5.1  3.5  1.4  0.2\n",
      "1    4.9  3.0  1.4  0.2\n",
      "2    4.7  3.2  1.3  0.2\n",
      "3    4.6  3.1  1.5  0.2\n",
      "4    5.0  3.6  1.4  0.2\n",
      "..   ...  ...  ...  ...\n",
      "145  6.7  3.0  5.2  2.3\n",
      "146  6.3  2.5  5.0  1.9\n",
      "147  6.5  3.0  5.2  2.0\n",
      "148  6.2  3.4  5.4  2.3\n",
      "149  5.9  3.0  5.1  1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert feature matrix into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# View the data frame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "certain-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  1.000000 -0.117570  0.871754  0.817941\n",
      "1 -0.117570  1.000000 -0.428440 -0.366126\n",
      "2  0.871754 -0.428440  1.000000  0.962865\n",
      "3  0.817941 -0.366126  0.962865  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "prerequisite-patio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create correlation heatmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Correlation Heatmap of Iris Dataset')\n",
    "a = sns.heatmap(corr_matrix, square=True, annot=True, fmt='.2f', linecolor='black')\n",
    "a.set_xticklabels(a.get_xticklabels(), rotation=30)\n",
    "a.set_yticklabels(a.get_yticklabels(), rotation=30)           \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "simplified-valuable",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-765c5967d629>:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.11757</td>\n",
       "      <td>0.871754</td>\n",
       "      <td>0.817941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.428440</td>\n",
       "      <td>-0.366126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.962865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0        1         2         3\n",
       "0 NaN -0.11757  0.871754  0.817941\n",
       "1 NaN      NaN -0.428440 -0.366126\n",
       "2 NaN      NaN       NaN  0.962865\n",
       "3 NaN      NaN       NaN       NaN"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "documented-dragon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# Find index of feature columns with correlation greater than 0.9\n",
    "\n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "obvious-laugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2\n",
      "0    5.1  3.5  1.4\n",
      "1    4.9  3.0  1.4\n",
      "2    4.7  3.2  1.3\n",
      "3    4.6  3.1  1.5\n",
      "4    5.0  3.6  1.4\n",
      "..   ...  ...  ...\n",
      "145  6.7  3.0  5.2\n",
      "146  6.3  2.5  5.0\n",
      "147  6.5  3.0  5.2\n",
      "148  6.2  3.4  5.4\n",
      "149  5.9  3.0  5.1\n",
      "\n",
      "[150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop Marked Features\n",
    "df1 = df.drop(df.columns[to_drop], axis=1)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-analyst",
   "metadata": {},
   "source": [
    "* We can see that we have dropped the third column from the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-renewal",
   "metadata": {},
   "source": [
    "## 3. Wrapper Methods \n",
    "\n",
    "In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from the subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are\n",
    "\n",
    "1. Forward selection,\n",
    "2. Backward elimination,\n",
    "3. Exhaustive feature selection,\n",
    "4. Recursive feature elimination.\n",
    "5. Recursive feature elimination with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-contents",
   "metadata": {},
   "source": [
    "### 3.1 Forward Selection \n",
    "\n",
    "* Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "* The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set.\n",
    "\n",
    "* The pre-set criteria can be the roc_auc for classification and the r squared for regression for example.\n",
    "\n",
    "* This selection procedure is called greedy, because it evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "hourly-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step forward feature selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "numerous-choice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('house_prices_advanced_regression_techniques_train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "wooden-tuesday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "explicit-tokyo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "united-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  3\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "coupled-sweet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 34), (438, 34))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "mediterranean-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "desirable-mongolia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:   37.8s finished\n",
      "\n",
      "[2021-06-02 22:32:37] Features: 1/10 -- score: 0.6672876190344853[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   46.4s finished\n",
      "\n",
      "[2021-06-02 22:33:23] Features: 2/10 -- score: 0.7236198589411359[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:   57.8s finished\n",
      "\n",
      "[2021-06-02 22:34:21] Features: 3/10 -- score: 0.7465002916075073[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  1.0min finished\n",
      "\n",
      "[2021-06-02 22:35:22] Features: 4/10 -- score: 0.7657699686817564[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   58.5s finished\n",
      "\n",
      "[2021-06-02 22:36:21] Features: 5/10 -- score: 0.7692729472663308[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:   52.2s finished\n",
      "\n",
      "[2021-06-02 22:37:13] Features: 6/10 -- score: 0.7716870975755739[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:   58.5s finished\n",
      "\n",
      "[2021-06-02 22:38:12] Features: 7/10 -- score: 0.7731123127245813[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  1.0min finished\n",
      "\n",
      "[2021-06-02 22:39:12] Features: 8/10 -- score: 0.7774320427971153[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  1.1min finished\n",
      "\n",
      "[2021-06-02 22:40:20] Features: 9/10 -- score: 0.8218400293003718[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  1.1min finished\n",
      "\n",
      "[2021-06-02 22:41:25] Features: 10/10 -- score: 0.8379175703119714"
     ]
    }
   ],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs1 = SFS(RandomForestRegressor(), \n",
    "           k_features=10, \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "patent-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9, 14, 15, 16, 17, 18, 19, 22, 24)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs1.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "loving-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OverallQual', 'BsmtFinSF1', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n",
       "       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'KitchenAbvGr',\n",
       "       'GarageCars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns[list(sfs1.k_feature_idx_)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-homework",
   "metadata": {},
   "source": [
    "* We can see that forward feature selection results in the above columns being selected from all the given columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-speaker",
   "metadata": {},
   "source": [
    "## 3.2 Backward Elimination \n",
    "\n",
    "\n",
    "* In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "* The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "considerable-camera",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:  3.3min finished\n",
      "\n",
      "[2021-06-02 22:50:23] Features: 33/10 -- score: 0.8562738445809811[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:  3.0min finished\n",
      "\n",
      "[2021-06-02 22:53:25] Features: 32/10 -- score: 0.8569273055282035[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  3.0min finished\n",
      "\n",
      "[2021-06-02 22:56:22] Features: 31/10 -- score: 0.8535998834523953[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  2.9min finished\n",
      "\n",
      "[2021-06-02 22:59:14] Features: 30/10 -- score: 0.8537285133492495[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.3s remaining:    0.0s\n",
      "\n",
      "STOPPING EARLY DUE TO KEYBOARD INTERRUPT..."
     ]
    }
   ],
   "source": [
    "sfs1 = SFS(RandomForestRegressor(), \n",
    "           k_features=10, \n",
    "           forward=False, ### forward = True for forward selection.\n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[list(sfs1.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-comment",
   "metadata": {},
   "source": [
    "## 3.3 Exhaustive Feature Selection\n",
    "Table of Contents\n",
    "\n",
    "In an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm. For example, if the classifier is a logistic regression and the dataset consists of 4 features, the algorithm will evaluate all 15 feature combinations as follows:\n",
    "\n",
    "* all possible combinations of 1 feature\n",
    "* all possible combinations of 2 features\n",
    "* all possible combinations of 3 features\n",
    "* all the 4 features\n",
    "\n",
    "\n",
    "and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.\n",
    "\n",
    "* This is another greedy algorithm as it evaluates all possible feature combinations. It is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "* There is a special package for python that implements this type of feature selection: mlxtend.\n",
    "\n",
    "* In the mlxtend implementation of the exhaustive feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.\n",
    "\n",
    "* This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-ecology",
   "metadata": {},
   "source": [
    "## 3.4 Recursive Feature elimination \n",
    "\n",
    "\n",
    "* It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "* Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2N combinations of features.\n",
    "\n",
    "<b>Source :</b> https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-shade",
   "metadata": {},
   "source": [
    "## 3.5 Recursive Feature Elimination with Cross-Validation \n",
    "\n",
    "\n",
    "* Recursive Feature Elimination with Cross-Validated (RFECV) feature selection technique selects the best subset of features for the estimator by removing 0 to N features iteratively using recursive feature elimination.\n",
    "\n",
    "* Then it selects the best subset based on the accuracy or cross-validation score or roc-auc of the model. Recursive feature elimination technique eliminates n features from a model by fitting the model multiple times and at each step, removing the weakest features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-spouse",
   "metadata": {},
   "source": [
    "# 4. Embedded Methods \n",
    "\n",
    "\n",
    "* Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n",
    "\n",
    "* This is why Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "\n",
    "* Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-inflation",
   "metadata": {},
   "source": [
    "## 4.1 LASSO Regression\n",
    "\n",
    "* Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "\n",
    "* Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularisation, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularisation, Lasso or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "spanish-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "copyrighted-statement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('house_prices_advanced_regression_techniques_train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "interstate-alarm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "steady-cheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "daily-index",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# the features in the house dataset are in very\n",
    "# different scales, so it helps the regression to scale them\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "indonesian-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=Lasso(alpha=100))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, again I will train a Lasso Linear regression and select\n",
    "# the non zero features in one line.\n",
    "# bear in mind that the linear regression object from sklearn does\n",
    "# not allow for regularisation. So If you want to make a regularised\n",
    "# linear regression you need to import specifically \"Lasso\"\n",
    "# that is the l1 version of the linear regression\n",
    "# alpha is the penalisation here, so I set it high in order\n",
    "# to force the algorithm to shrink some coefficients\n",
    "\n",
    "sel_ = SelectFromModel(Lasso(alpha=100))\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bound-judges",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "unavailable-grammar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 37\n",
      "selected features: 33\n",
      "features with coefficients shrank to zero: 4\n"
     ]
    }
   ],
   "source": [
    "# make a list with the selected features and print the outputs\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-barbados",
   "metadata": {},
   "source": [
    "* We can see that Lasso regularisation helps to remove non-important features from the dataset. So, increasing the penalisation will result in increase the number of features removed. Therefore, we need to keep an eye and monitor that we don't set a penalty too high so that to remove even important features, or too low and then not remove non-important features.\n",
    "\n",
    "* If the penalty is too high and important features are removed, we will notice a drop in the performance of the algorithm and then realise that we need to decrease the regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-armor",
   "metadata": {},
   "source": [
    "# 4.2 Random Forest Importance\n",
    "\n",
    "* Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "* Random forests consist of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or combination of features. At each node (this is at each question), the three divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is.\n",
    "\n",
    "* For classification, the measure of impurity is either the Gini impurity or the information gain/entropy. For regression the measure of impurity is variance. Therefore, when training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "* To give a better intuition, features that are selected at the top of the trees are in general more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "available-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Load dataset\n",
    "df = pd.read_csv('mushrooms.csv')\n",
    "# Declare feature vector and target variable\n",
    "X = df.drop(['class'], axis = 1)\n",
    "y = df['class']\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, prefix_sep='_')\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "\n",
    "# Normalize feature vector\n",
    "X2 = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "# instantiate the classifier with n_estimators = 100\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# fit the classifier to the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-austin",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "* Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importance of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it’s predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "frozen-diversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x640 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize feature importance\n",
    "\n",
    "plt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "feat_importances = pd.Series(clf.feature_importances_, index= X.columns)\n",
    "\n",
    "feat_importances.nlargest(7).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-glance",
   "metadata": {},
   "source": [
    "* Now we know which features are most important in the Random Forest model, we can train our model just using these features.\n",
    "\n",
    "* I have implemented this in the kernel - Random Forest Classifier + Feature Importance : Section 15 - Build the Random Forest model on selected features. It resulted in improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-indie",
   "metadata": {},
   "source": [
    "## Numerical Input, Numerical Output\n",
    "\n",
    "* This is a regression predictive modeling problem with numerical input variables.\n",
    "\n",
    "* The most common techniques are to use a correlation coefficient, such as Pearson’s for a linear correlation, or rank-based methods for a nonlinear correlation.\n",
    "\n",
    "* The tests emplyed are as follows:-\n",
    "\n",
    "    * Pearson’s correlation coefficient (linear).\n",
    "    * Spearman’s rank coefficient (nonlinear)\n",
    "\n",
    "## Numerical Input, Categorical Output\n",
    "\n",
    "* This is a classification predictive modeling problem with numerical input variables.\n",
    "\n",
    "* This might be the most common example of a classification problem,\n",
    "\n",
    "* Again, the most common techniques are correlation based, although in this case, they must take the categorical target into account.\n",
    "\n",
    "* We can employ the following tests as follows:-\n",
    "\n",
    "    * ANOVA correlation coefficient (linear).\n",
    "    * Kendall’s rank coefficient (nonlinear).\n",
    "    * Kendall does assume that the categorical variable is ordinal.\n",
    "\n",
    "## Categorical Input, Numerical Output\n",
    "\n",
    "* This is a regression predictive modeling problem with categorical input variables.\n",
    "\n",
    "* This is a strange example of a regression problem (e.g. we will not encounter it often).\n",
    "\n",
    "* We can use the same “Numerical Input, Categorical Output” methods (described above), but in reverse.\n",
    "\n",
    "## Categorical Input, Categorical Output\n",
    "\n",
    "* This is a classification predictive modeling problem with categorical input variables.\n",
    "\n",
    "* The most common correlation measure for categorical data is the chi-squared test. We can also use mutual information (information gain) from the field of information theory.\n",
    "\n",
    "* The following tests can be employed in this case -\n",
    "\n",
    "    * Chi-Squared test (contingency tables).\n",
    "    * Mutual Information.\n",
    "\n",
    "* In fact, mutual information is a powerful method that may prove useful for both categorical and numerical data, e.g. it is agnostic to the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-aberdeen",
   "metadata": {},
   "source": [
    "# 6. Tips and Tricks for Feature Selection \n",
    "\n",
    "\n",
    "### Correlation Statistics\n",
    "\n",
    "* The scikit-learn library provides an implementation of most of the useful statistical measures.\n",
    "\n",
    "* For example:\n",
    "\n",
    "    * Pearson’s Correlation Coefficient: f_regression()\n",
    "    * ANOVA: f_classif()\n",
    "    * Chi-Squared: chi2()\n",
    "    * Mutual Information: mutual_info_classif() and mutual_info_regression().\n",
    "\n",
    "* Also, the SciPy library provides an implementation of many more statistics, such as Kendall’s tau (kendalltau) and Spearman’s rank correlation (spearmanr).\n",
    "\n",
    "\n",
    "### Selection Method\n",
    "\n",
    "* The scikit-learn library also provides many different filtering methods once statistics have been calculated for each input variable with the target.\n",
    "\n",
    "* Two of the more popular methods include:\n",
    "\n",
    "* Select the top k variables: SelectKBest\n",
    "* Select the top percentile variables: SelectPercentile\n",
    "\n",
    "### Transform Variables\n",
    "\n",
    "* We can consider transforming the variables in order to access different statistical methods. For example, we can transform a categorical variable to ordinal, even if it is not, and see if any interesting results come out.\n",
    "\n",
    "* We can also make a numerical variable discrete (e.g. bins); try categorical-based measures.\n",
    "\n",
    "* Some statistical measures assume properties of the variables, such as Pearson’s that assumes a Gaussian probability distribution to the observations and a linear relationship. You can transform the data to meet the expectations of the test and try the test regardless of the expectations and compare results.\n",
    "\n",
    "\n",
    "### 4 best ways of Feature Selection\n",
    "\n",
    "The 4 practical ways of feature selection which yield best results are as follows:-\n",
    "\n",
    "1. SelectKBest\n",
    "2. Recursive Feature Elimination\n",
    "3. Correlation-matrix with heatmap\n",
    "4. Random-Forest Importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
